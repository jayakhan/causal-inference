{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook scrapes all listings.csv.gz united-states data from the insideairbnb get data page and combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read html page with links to each indivdual csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../00_data/Get the Data - Inside Airbnb. Adding data to the debate..html\", encoding='utf8') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantlines =  [line for line in lines if bool(re.search(r\"listings\\.csv\\.gz\\\"\", line))] #subset lines with listings.csv but not gz\n",
    "links = [re.findall(r\"href=\\\".*listings\\.csv\\.gz\\\"\", line)[0][6:-1] for line in relevantlines] #extract the links\n",
    "links = [l for l in links if bool(re.search(r\"united-states\", l)) ] #subset for united-states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download files to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satvi\\Anaconda3\\envs\\ids701\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (67) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for i,link in enumerate(links):\n",
    "    try:\n",
    "        files.append(pd.read_csv(link))\n",
    "        del files[-1]['license']\n",
    "        pass\n",
    "    except:\n",
    "        files.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean out links that did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_links = [i for i,f in enumerate(files) if f is None]\n",
    "links = [link for i,link in enumerate(links) if i not in bad_links]\n",
    "files = [file for i,file in enumerate(files) if i not in bad_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract file information and add to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [link.split(\"/\")[-4] for link in links ]\n",
    "dates = [link.split(\"/\")[-3] for link in links ]\n",
    "dates = [int(date.split(\"-\")[1]) for date in dates]\n",
    "for i,file in enumerate(files):\n",
    "    file['region'] = regions[i]\n",
    "    file['month'] = dates[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Columns:  {'review_scores_checkin', 'availability_365', 'neighbourhood', 'availability_60', 'number_of_reviews', 'number_of_reviews_ltm', 'review_scores_value', 'host_id', 'review_scores_location', 'property_type', 'longitude', 'host_response_time', 'calculated_host_listings_count', 'maximum_nights', 'beds', 'host_is_superhost', 'review_scores_communication', 'host_about', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_shared_rooms', 'amenities', 'price', 'name', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'review_scores_cleanliness', 'host_identity_verified', 'host_since', 'number_of_reviews_l30d', 'listing_url', 'host_listings_count', 'neighbourhood_cleansed', 'maximum_minimum_nights', 'host_total_listings_count', 'host_verifications', 'calendar_updated', 'review_scores_accuracy', 'last_scraped', 'scrape_id', 'availability_90', 'host_location', 'host_thumbnail_url', 'last_review', 'neighborhood_overview', 'host_neighbourhood', 'has_availability', 'first_review', 'bedrooms', 'review_scores_rating', 'host_name', 'bathrooms', 'calendar_last_scraped', 'instant_bookable', 'reviews_per_month', 'minimum_minimum_nights', 'calculated_host_listings_count_private_rooms', 'host_picture_url', 'region', 'bathrooms_text', 'minimum_nights', 'id', 'host_acceptance_rate', 'description', 'month', 'host_has_profile_pic', 'availability_30', 'host_url', 'neighbourhood_group_cleansed', 'maximum_nights_avg_ntm', 'host_response_rate', 'picture_url', 'room_type', 'minimum_maximum_nights', 'latitude', 'accommodates'}\n",
      "Unique number of Columns:  {75}\n"
     ]
    }
   ],
   "source": [
    "unique_columns = set() #All unique column names\n",
    "unique_n_columns = set() #All unique number of columns per dataset\n",
    "for file in files:\n",
    "    unique_n_columns.add(len(file.columns.values))\n",
    "    for col in file.columns.values:\n",
    "        unique_columns.add(col)\n",
    "        pass\n",
    "    pass\n",
    "print(\"Unique Columns: \", unique_columns)\n",
    "print(\"Unique number of Columns: \", unique_n_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that can go missing are:  set()\n"
     ]
    }
   ],
   "source": [
    "missing_cols = [[col for col in unique_columns if col not in f.columns.values] for f in files]\n",
    "missing_cols = [mc for mc in missing_cols if len(mc) != 0]\n",
    "missing_cols_set = set()\n",
    "for mcs in missing_cols:\n",
    "    for mc in mcs:\n",
    "        missing_cols_set.add(mc)\n",
    "print(\"Columns that can go missing are: \", missing_cols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we observe all indivual files to have the same 76 columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv = pd.concat(files, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv.to_csv(\"../00_data/combined.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdaef2cc1f35aaec713d791e8978da65b7e43f4b4f1bd940dca1c823e96286d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
